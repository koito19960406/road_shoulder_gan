import os
import matplotlib.pyplot as plt
import re
import seaborn as sn
import pandas as pd
import sys
from plotnine import ggplot, aes, geom_tile, scale_fill_gradient, theme, element_text

class TrainHistoryVisualizer:
    def __init__(self,root_dir):
        self.root_dir = root_dir
        
    def cyclegan_generate_stats_from_log(self, experiment_name, line_interval=10, nb_data=10800, enforce_last_line=True):
        """
        Generate chart with all losses from log file generated by CycleGAN/Pix2pix/CUT framework
        """
        # run if the output doesn't exist yet
        os.makedirs(f"./reports/figures/{experiment_name}", exist_ok = True)
        if os.path.exists(f"./reports/figures/{experiment_name}/train_loss.png"):
            print(f"./reports/figures/{experiment_name}/train_loss.png" + " already exists")
        else:
            #extract every lines
            with open(os.path.join(self.root_dir, "models", experiment_name, "loss_log.txt"), 'r') as f:
                lines = f.readlines()
            #choose the lines to use for plotting
            lines_for_plot = []
            for i in range(1,len(lines)):
                if (i-1) % line_interval==0:
                    lines_for_plot.append(lines[i])
            if enforce_last_line:
                lines_for_plot.append(lines[-1])
            #initialize dict with loss names
            dicts = dict()
            dicts["epoch"] = []
            parts = (lines_for_plot[0]).split(') ')[1].split(' ')
            for i in range(0, len(parts)//2):
                dicts[parts[2*i][:-1]] = []
            #extract all data
            pattern = "epoch: ([0-9]+), iters: ([0-9]+)"
            for l in lines_for_plot:
                search = re.search(pattern, l)
                try:
                    epoch = int(search.group(1))
                except:
                    continue
                epoch_floatpart = int(search.group(2))/nb_data
                dicts["epoch"].append(epoch+epoch_floatpart) #to allow several plots for the same epoch
                parts = l.split(') ')[1].split(' ')
                for i in range(0, len(parts)//2):
                    try:
                        dicts[parts[2*i][:-1]].append(float(parts[2*i+1]))
                    except:
                        dicts[parts[2*i][:-1]] = []
                        dicts[parts[2*i][:-1]].append(float(parts[2*i+1]))
            #plot everything
            plt.figure()
            for key in dicts.keys():
                # if (key != "epoch") & (key=="cycle_A"):
                if "_A" in key:
                    if "cycle" in key:
                        line_width = 1
                    else:
                        line_width =0.3
                    plt.plot(dicts["epoch"], dicts[key], label=key, linewidth=line_width)
            plt.legend(loc="best")
            plt.savefig(f"./reports/figures/{experiment_name}/train_loss.png", figsize=(3, 6))
            plt.clf()
            # plt.show()
            # plt.close()
            
    def pix2pix_generate_stats_from_log(self, experiment_name, line_interval=10, nb_data=10800, enforce_last_line=True):
        """
        Generate chart with all losses from log file generated by CycleGAN/Pix2pix/CUT framework
        """
        # run if the output doesn't exist yet
        os.makedirs(f"./reports/figures/{experiment_name}", exist_ok = True)
        if (os.path.exists(f"./reports/figures/{experiment_name}/gen_train_loss.png"))&\
            (os.path.exists(f"./reports/figures/{experiment_name}/det_train_loss.png")):
            print("Pix2Pix plot already exists")
        else:
            #extract every lines
            with open(os.path.join(self.root_dir, "models", experiment_name, "loss_log.txt"), 'r') as f:
                lines = f.readlines()
            #choose the lines to use for plotting
            lines_for_plot = []
            for i in range(1,len(lines)):
                if (i-1) % line_interval==0:
                    lines_for_plot.append(lines[i])
            if enforce_last_line:
                lines_for_plot.append(lines[-1])
            #initialize dict with loss names
            dicts = dict()
            dicts["epoch"] = []
            parts = (lines_for_plot[0]).split(') ')[1].split(' ')
            for i in range(0, len(parts)//2):
                dicts[parts[2*i][:-1]] = []
            #extract all data
            pattern = "epoch: ([0-9]+), iters: ([0-9]+)"
            for l in lines_for_plot:
                search = re.search(pattern, l)
                try:
                    epoch = int(search.group(1))
                except:
                    continue
                epoch_floatpart = int(search.group(2))/nb_data
                dicts["epoch"].append(epoch+epoch_floatpart) #to allow several plots for the same epoch
                parts = l.split(') ')[1].split(' ')
                for i in range(0, len(parts)//2):
                    try:
                        dicts[parts[2*i][:-1]].append(float(parts[2*i+1]))
                    except:
                        dicts[parts[2*i][:-1]] = []
                        dicts[parts[2*i][:-1]].append(float(parts[2*i+1]))
            #plot generator loss
            plt.figure()
            for key in dicts.keys():
                if (key != "epoch")&("D_" not in key):
                    plt.plot(dicts["epoch"], dicts[key], label=key, linewidth=0.5)
            plt.legend(loc="best")
            plt.savefig(f"./reports/figures/{experiment_name}/gen_train_loss.png", figsize=(3, 6))
            plt.clf()
            
            #plot detector loss
            plt.figure()
            for key in dicts.keys():
                if (key != "epoch")&("G_" not in key):
                    plt.plot(dicts["epoch"], dicts[key], label=key, linewidth=0.5)
            plt.legend(loc="best")
            plt.savefig(f"./reports/figures/{experiment_name}/det_train_loss.png", figsize=(3, 6))
            plt.clf()
            
class SegCorrVisualizer:
    """class to visualize correlation matrix of segmentation results
    """
    
    def __init__(self, input_file, output_file, label_list):
        self.input_file = input_file
        self.output_file = output_file
        self.label_list = label_list
        
    def plot(self):
        # read csv and retain all the columns except for pid col (first col)
        df = pd.read_csv(self.input_file).iloc[:,1:]
        for label in self.label_list:
            df_filtered = df.filter(regex=label.name)
            if len(df_filtered.columns) > 0:
                corrMatrix = df_filtered.corr()
                fig, ax = plt.subplots()
                sn.set(rc={"figure.figsize":(8, 8)})
                sn.heatmap(corrMatrix, annot = True)
                plt.xticks(rotation=90, size = 20)
                plt.yticks(rotation=0, size = 20)
                fig.tight_layout()
                plt.savefig(os.path.join(self.output_file, f"segmentation_correlation_matrix_{label.name}.pdf"))
                plt.clf()

# class SegCorrVisualizer:
#     """class to visualize correlation matrix of segmentation results using ggplot
#     """
    
#     def __init__(self, input_file, output_file):
#         self.input_file = input_file
#         self.output_file = output_file
        
#     def plot(self):
#         # read csv and retain all the columns except for pid col (first col)
#         df = pd.read_csv(self.input_file).iloc[:,1:]
#         # get unique list of column names that contain either one of "gsv_", "mly_", "gan_" and replace them with empty string
#         unique_col_names = [re.sub(r"(gsv_|mly_|gan_)","",col) for col in df.columns if re.search(r"(gsv_|mly_|gan_)",col)]
#         for label in unique_col_names:
#             df_filtered = df.filter(regex=label)
#             if len(df_filtered.columns) > 0:
#                 corrMatrix = df_filtered.corr()
                
#                 # Melt the correlation matrix for ggplot
#                 corr_melted = corrMatrix.reset_index().melt(id_vars='index')
#                 corr_melted.columns = ['x', 'y', 'value']
                
#                 # Plot using ggplot
#                 p = (ggplot(corr_melted, aes(x='x', y='y', fill='value')) +
#                      geom_tile() +
#                      scale_fill_gradient(low="white", high="blue") +
#                      theme(axis_text_x=element_text(rotation=90, size=10),
#                            axis_text_y=element_text(size=10)))
                
#                 # Save the plot
#                 p.save(os.path.join(self.output_file, f"segmentation_correlation_matrix_{label}.png"))

        
if __name__ == '__main__':
    root_dir = "./"
    drives = [f"{chr(x)}:/" for x in range(ord('D'), ord('Z') + 1)] + ["/Volumes/ExFAT/"]
    for drive in drives:
        if os.path.exists(os.path.join(drive,"road_shoulder_gan")):
            root_dir = os.path.join(drive,"road_shoulder_gan")
            break
    # train loss plot
    for experiment_name in os.listdir(os.path.join(root_dir,"models")):
        if not experiment_name.startswith('.'):  
            # visualizer = TrainHistoryVisualizer(root_dir)
            # if "cyclegan" in experiment_name:
            #     visualizer.cyclegan_generate_stats_from_log(experiment_name, line_interval=10, nb_data=3473)
            # if "pix2pix" in experiment_name:
            #     visualizer.pix2pix_generate_stats_from_log(experiment_name, line_interval=10, nb_data=3473)
    
            # segmentation result correlation plot
            input_file = os.path.join(root_dir, "models",experiment_name,"segmentation_result/segmentation_result.csv")
            output_folder = f"./reports/figures/{experiment_name}"
            segcorr = SegCorrVisualizer(input_file, output_folder)
            try:
                segcorr.plot()
            except Exception as e:
                print(e)
                pass