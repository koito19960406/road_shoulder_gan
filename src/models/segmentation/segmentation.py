# from curses import def_prog_mode
from torch.utils.data import Dataset, DataLoader
from transformers import AdamW
import torch
from torch import nn
from sklearn.metrics import accuracy_score
from tqdm.notebook import tqdm
import os
from PIL import Image
from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor, SegformerConfig
import pandas as pd
import cv2
import numpy as np
import albumentations as aug
from transformers import DetrFeatureExtractor, DetrForObjectDetection
import glob
import re
import tqdm
import json
import shutil
from transformers import DetrFeatureExtractor, DetrForSegmentation
# from detectron2.data import MetadataCatalog
from transformers.models.detr.feature_extraction_detr import rgb_to_id, id_to_rgb
import io
from copy import deepcopy
# from detectron2.utils.visualizer import Visualizer
from multiprocessing.pool import ThreadPool
from threading import Thread
from queue import Queue
import csv 
from pyarrow import feather
import dask
import dask.dataframe as dd

from .datasets.simple_segmentation import SimpleSegmentationInferDataset
from .color_maps import cityscapes

# def filter_image(gsv_metadata_folder, input_folder):
#     """function to filter out invalid images and save as csv to avoid errors later
#     """
#     if not os.path.exists(os.path.join(gsv_metadata_folder,"invalid_file_name.csv")):
#         invalid_list = []
#         for image_file in tqdm.tqdm(os.listdir(input_folder), total = len(os.listdir(input_folder))):
#             try:
#                 image = cv2.imread(os.path.join(input_folder,image_file))
#                 if image is None:
#                     invalid_list.append(image_file)
                        
#             except cv2.error:
#                 invalid_list.append(image_file)
#         # save the list as csv
#         invalid_df = pd.DataFrame(invalid_list, columns=["file_name"])
#         invalid_df.to_csv(os.path.join(gsv_metadata_folder,"invalid_file_name.csv"), index = False)
#         print("-"*10, "removed ", str(len(invalid_df.index)), " invalid image files", "-"*10)
#     else:
#         print("Following file already exists: ", os.path.join(gsv_metadata_folder,"invalid_file_name.csv"))
        
def move_results(result_folder, new_folder):
    """This function moves fake images generated by gan models to a new folder to facilitate analysis later

    Args:
        result_folder (_type_): folder with gan results
        new_folder (_type_): new folder to store only fake images
    """
    os.makedirs(new_folder, exist_ok = True)
    # get a list of fake img
    fake_img_list = glob.glob(os.path.join(result_folder,"*_fake_B.png"))
    # copy to the new folder
    for fake_img in tqdm.tqdm(fake_img_list):
        fake_img_file_name = os.path.basename(fake_img).replace("_fake_B.png","") + ".png"
        shutil.copy2(fake_img,os.path.join(new_folder,fake_img_file_name))
        
class ImageSegmentationSimple:
    def __init__(self, input_folder, img_output_folder, csv_output_folder):
        self.input_folder = input_folder
        self.img_output_folder = img_output_folder
        self.csv_output_folder = csv_output_folder
        # self.final_output_folder = final_output_folder
        # create folder if it doesn't exist yet
        os.makedirs(self.img_output_folder, exist_ok = True)
        os.makedirs(self.csv_output_folder, exist_ok = True)
        # set the device
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.cpu_num = os.cpu_count()
        self.labels = cityscapes.create_cityscapes_label_colormap()
        # filter image to remove invalid images
        # filter_image(self.gsv_metadata_folder, self.input_folder)
            
    def segment_svi(self, pretrained_model = "nvidia/segformer-b5-finetuned-cityscapes-1024-1024",batch_size_store=500):
        """function to conduct segmentation
        """
        # import segmentation model
        feature_extractor = SegformerFeatureExtractor()
        model = SegformerForSemanticSegmentation.from_pretrained(pretrained_model).to(self.device)
        
        infer_data = SimpleSegmentationInferDataset(self.input_folder, feature_extractor, self.csv_output_folder)
        infer_loader = torch.utils.data.DataLoader(infer_data,
                                                batch_size=1, # keep this to 1 to avoid errors
                                                shuffle=False,
                                                num_workers=4,
                                                pin_memory=True)
        print(len(infer_loader))
        assert ((len(infer_loader) == 0)|(len(infer_loader) >= 10)), f"Your data size is {str(len(infer_loader))} (less than 10), this will not work due to feather format's feature"
        with torch.no_grad():
            # batch count to bundle result into feather file
            # save to feather every 500 count
            feather_file_num = len(glob.glob(os.path.join(self.csv_output_folder,"*.feather")))
            if feather_file_num==0:
                batch_count = 1
            else:
                batch_count=feather_file_num*batch_size_store+1
            # initialize df to save the result
            result_list_agg = []
            for inputs, file_name in tqdm.tqdm(infer_loader, total = len(infer_loader)):
                # produce logits from the model
                inputs = inputs.to(self.device)
                outputs = model(pixel_values = inputs)
                logits = outputs.logits.cpu()
                upsampled_logits = nn.functional.interpolate(logits,
                    size=inputs.shape[-2:], # (height, width)
                    mode='bilinear',
                    align_corners=False)

                # Second, apply argmax on the class dimension
                seg = upsampled_logits.argmax(dim=1)[0].numpy()
                # calculate the total pixels
                total_pixel = seg.shape[0] * seg.shape[1]
                # Finally we visualize the prediction
                color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)
                # get an array of unique ids
                unique_id_array = np.unique(seg)
                result_list_agg_temp = []
                for unique_id in unique_id_array:
                    # filter to get a respective label
                    label = list(filter(lambda label: label.trainId == unique_id, self.labels))[0]
                    # give color to color_seg
                    color_seg[seg == unique_id, :] = label.color
                    # calculate the number of pixel for each class
                    pixel_num = np.sum((seg==unique_id))
                    pixel_ratio = pixel_num / total_pixel
                    result_list_agg_temp.append([file_name[0], label.name, pixel_ratio])
                # if there's no result, then append only pid
                if len(result_list_agg_temp) == 0:
                    result_list_agg_temp.append([file_name[0],None,None])
                # append result_list_agg_temp to result_list_agg
                result_list_agg.extend(result_list_agg_temp)
                if batch_count % batch_size_store == 0:
                    # convert the result_list_agg to df and save as csv
                    result_list_agg_df = pd.DataFrame(result_list_agg, columns=["file_name","name","pixel_ratio"])
                    result_list_agg_df.to_feather(os.path.join(self.csv_output_folder, f"segmentation_{str(batch_count)}.feather"))
                    result_list_agg = []
                # addd 1 to batch_count
                batch_count += 1
                # save it to png
                cv2.imwrite(os.path.join(self.img_output_folder, file_name[0]+".png"), cv2.cvtColor(color_seg, cv2.COLOR_RGB2BGR))
            # save the remaining data to the latest feather file
            if len(result_list_agg) != 0:
                result_list_agg_df = pd.DataFrame(result_list_agg, columns=["file_name","name","pixel_ratio"])
                if batch_count < batch_size_store:
                    result_list_agg_df.to_feather(os.path.join(self.csv_output_folder, f"object_detection_{str(batch_count)}.feather"))
                else:
                    latest_batch_num = (batch_count // batch_size_store) * batch_size_store
                    latest_df = pd.read_feather(os.path.join(self.csv_output_folder, f"segmentation_{str(latest_batch_num)}.feather"))
                    result_list_agg_df = pd.concat([latest_df, result_list_agg_df], ignore_index = True)
                    result_list_agg_df.to_feather(os.path.join(self.csv_output_folder, f"segmentation_{str(latest_batch_num)}.feather"))

        
    def calculate_ratio(self, update = False):
        # transform the csv to wide format
        feather_list = glob.glob(os.path.join(self.csv_output_folder, "*.feather"))
        dfs = [dask.delayed(feather.read_feather)(f) for f in feather_list]
        df = dd.from_delayed(dfs)
        segment_pixel_ratio_df = df.drop_duplicates().compute()
        segment_pixel_ratio_df = (segment_pixel_ratio_df.pivot_table(index="file_name", columns= "name", values='pixel_ratio', aggfunc=np.mean).
                                        reset_index().
                                        fillna(0))
        # save as csv
        segment_pixel_ratio_df.to_csv(os.path.join(self.csv_output_folder, "segmentation_pixel_ratio_wide.csv"), index = False)

class CorrelationAnalysis:
    """class for analyzing correlation result
    """
    
    def __init__(self,gsv_result_csv, mly_result_csv, gan_result_csv, output_folder):
        self.gsv_result_csv = gsv_result_csv
        self.mly_result_csv = mly_result_csv
        self.gan_result_csv = gan_result_csv
        self.output_folder = output_folder
        
    def load_rename(self, file_path, prefix):
        """load csv as df and rename col names

        Args:
            file_path (str): file path to the segmentation result
            prefix (str): prefix to add to col names
        """
        result = pd.read_csv(file_path)
        # rename all the cols excepts for the first column, which is pid
        result.columns = [prefix + colname if i>0 else colname for i, colname in enumerate(result.columns)]
        return result

    def merge_save(self):
        """merge and save the results
        """
        # load
        gsv_result = self.load_rename(self.gsv_result_csv, "gsv_")
        mly_result = self.load_rename(self.mly_result_csv, "mly_")
        gan_result = self.load_rename(self.gan_result_csv, "gan_")
        # merge
        merged_result = (gsv_result.merge(mly_result,on=["file_name"],how="left").
            merge(gan_result,on=["file_name"],how="left")
            )
        # save as csv
        merged_result.to_csv(os.path.join(self.output_folder,"segmentation_result.csv"))

if __name__ == '__main__':
    root_dir = "/Volumes/ExFAT/road_shoulder_gan"
    # copuy results to new folders
    name_list = ["cyclegan_filtered", "pix2pix_filtered"]
    for name in name_list:
        result_folder = os.path.join(root_dir, f"models/{name}/{name}/test_latest/images")
        new_folder = os.path.join(root_dir, f"data/processed/{name}/results")
        if not os.path.exists(new_folder):
            move_results(result_folder, new_folder)
        # segment input (mly and gsv) and output (fake) 
        mly_input_folder = os.path.join(root_dir,"data/processed/cyclegan_filtered/testB") 
        gsv_input_folder = os.path.join(root_dir,"data/processed/cyclegan_filtered/testA") 
        for input_folder in [new_folder, mly_input_folder, gsv_input_folder]:
            img_type = os.path.basename(input_folder)
            img_output_folder = os.path.join(root_dir, "data/interim/segmented",name,img_type)
            csv_output_folder = os.path.join(root_dir, "data/processed",name,"segmentation_result",img_type)
            segmentation = ImageSegmentationSimple(input_folder, img_output_folder, csv_output_folder)
            segmentation.segment_svi()
            segmentation.calculate_ratio()
        gsv_result_csv = os.path.join(root_dir, "data/processed",name,"segmentation_result/testA/segmentation_pixel_ratio_wide.csv")
        mly_result_csv = os.path.join(root_dir, "data/processed",name,"segmentation_result/testB/segmentation_pixel_ratio_wide.csv")
        gan_result_csv = os.path.join(root_dir, "data/processed",name,"segmentation_result/results/segmentation_pixel_ratio_wide.csv")
        output_folder = os.path.join(root_dir, "data/processed",name,"segmentation_result")
        correlation_analysis = CorrelationAnalysis(gsv_result_csv, mly_result_csv, gan_result_csv, output_folder)
        correlation_analysis.merge_save()
    pass